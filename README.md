# AI-Based Longitudinal Analysis of Post-COVID-19 Lung Recovery Using Deformable CT Image Registration

A deep learning pipeline for tracking post-COVID-19 lung recovery over time using **deformable image registration** on chest CT scans. The system registers follow-up CT scans to baseline scans using a **VoxelMorph** architecture, produces dense displacement fields, and quantifies tissue recovery through **Jacobian analysis**, **density tracking**, and **structural similarity scoring**.

**Optimized for NVIDIA H200 GPU** (141 GB HBM3e, BF16 Tensor Cores, Hopper Architecture).

---

## ðŸ—ï¸ Architecture

```
Input: (Moving CT, Fixed CT) â†’ U-Net â†’ Velocity Field â†’ Scaling & Squaring â†’ Displacement Field â†’ Spatial Transformer â†’ Warped CT
                                                                                   â†“
                                                                         Jacobian Analysis â†’ Recovery Scoring
```

### Key Components:
| Component | Description |
|-----------|-------------|
| **3D U-Net Backbone** | Encoder-decoder with skip connections for displacement/velocity field prediction |
| **Spatial Transformer** | Differentiable 3D warping with trilinear interpolation |
| **Diffeomorphic Integration** | Scaling-and-squaring for topology-preserving transforms |
| **Recovery Analyzer** | Jacobian-based volume change scoring with regional analysis |

---

## ðŸ“‚ Project Structure

```
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ default.yaml              # Default hyperparameters
â”‚   â””â”€â”€ h200_optimized.yaml       # H200 GPU-specific overrides
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ download_datasets.py      # Dataset download & preparation
â”‚   â”œâ”€â”€ preprocessing.py          # CT windowing, resampling, normalization
â”‚   â”œâ”€â”€ lung_segmentation.py      # Automated lung ROI extraction
â”‚   â”œâ”€â”€ dataset.py                # PyTorch Dataset + DataLoaders
â”‚   â””â”€â”€ augmentation.py           # 3D spatial + intensity augmentation
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ unet3d.py                 # 3D U-Net encoder-decoder
â”‚   â”œâ”€â”€ spatial_transformer.py    # STN + diffeomorphic integration
â”‚   â”œâ”€â”€ voxelmorph.py             # VoxelMorph & VoxelMorph-Diff models
â”‚   â”œâ”€â”€ losses.py                 # NCC, SSIM, bending energy, Jacobian
â”‚   â””â”€â”€ recovery_analyzer.py      # Longitudinal recovery scoring
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train.py                  # Main training script
â”‚   â”œâ”€â”€ trainer.py                # Training loop (H200 optimized)
â”‚   â””â”€â”€ lr_scheduler.py           # Warmup + cosine annealing
â”œâ”€â”€ inference/
â”‚   â”œâ”€â”€ register.py               # Registration inference
â”‚   â”œâ”€â”€ analyze_recovery.py       # Longitudinal analysis
â”‚   â””â”€â”€ visualize.py              # Visualization suite
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ metrics.py                # Dice, TRE, SSIM, Jacobian stats
â”‚   â”œâ”€â”€ io_utils.py               # File I/O helpers
â”‚   â””â”€â”€ logging_utils.py          # TensorBoard + W&B
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_model.py             # Model forward pass tests
â”‚   â”œâ”€â”€ test_losses.py            # Loss function tests
â”‚   â””â”€â”€ test_data.py              # Data pipeline tests
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ðŸ“Š Datasets

| Dataset | Size | Source | Use |
|---------|------|--------|-----|
| **STOIC** | 2,000 CT scans | [grand-challenge.org](https://stoic2021.grand-challenge.org/) | Primary training |
| **COVID-CT+** | 400K+ images | [NIH](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8411519/) | Supplementary |
| **BIMCV COVID-19+** | Large annotated | [bimcv.cipf.es](https://bimcv.cipf.es/bimcv-projects/bimcv-covid19/) | Validation |

### Synthetic Longitudinal Pair Generation
Since true public longitudinal COVID-CT datasets are scarce, the pipeline includes synthetic pair generation that simulates recovery by applying controlled deformations and density changes with known ground-truth displacement fields.

---

## ðŸš€ Quick Start

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Generate Demo Data (for testing)
```bash
python data/download_datasets.py --action demo --num-demo 20 --demo-size 64
```

### 3. Generate Synthetic Pairs
```bash
python data/download_datasets.py --action synthetic --num-pairs 5
```

### 4. Smoke Test (5 epochs on demo data)
```bash
python training/train.py --config configs/default.yaml --smoke-test
```

### 5. Full Training (H200 Optimized)
```bash
python training/train.py \
    --config configs/default.yaml \
    --override configs/h200_optimized.yaml \
    --data-dir ./datasets
```

### 6. Inference
```bash
# Register a pair
python inference/register.py \
    --checkpoint outputs/checkpoints/best_model.pth \
    --moving path/to/baseline.nii.gz \
    --fixed path/to/followup.nii.gz

# Longitudinal analysis
python inference/analyze_recovery.py \
    --checkpoint outputs/checkpoints/best_model.pth \
    --patient-dir datasets/patient_001/ \
    --timepoint-labels "Baseline" "3 Months" "6 Months" "12 Months"

# Generate visualizations
python inference/visualize.py --results-dir results/
```

---

## âš¡ H200 GPU Optimizations

| Feature | Setting | Benefit |
|---------|---------|---------|
| **BF16 Mixed Precision** | Native Hopper BF16 Tensor Cores | 2Ã— throughput vs FP32 |
| **TF32 Matmul** | `torch.set_float32_matmul_precision('high')` | Faster FP32 operations |
| **`torch.compile()`** | Graph-mode optimization | Kernel fusion for Hopper |
| **cuDNN Benchmark** | `cudnn.benchmark = True` | Optimal conv algorithms |
| **Large Batch Size** | 8 (Ã—4 accumulation = 32 effective) | Leverages 141 GB HBM3e |
| **Multi-worker DataLoader** | 16 workers, prefetch_factor=4 | Saturates 4.8 TB/s bandwidth |
| **Gradient Accumulation** | 4 steps | Effective batch size 32 |

---

## ðŸ“ˆ Loss Functions

**Total Loss = Î»_sim Ã— L_sim + Î»_smooth Ã— L_smooth + Î»_jac Ã— L_jac**

| Loss | Purpose | Default Weight |
|------|---------|----------------|
| **NCC** (Normalized Cross-Correlation) | Image similarity | 1.0 |
| **Bending Energy** | Deformation smoothness | 3.0 |
| **Jacobian Determinant** | Topology preservation | 0.1 |
| **Dice** (optional) | Segmentation alignment | 0.5 |

---

## ðŸ”¬ Recovery Analysis

The recovery analyzer quantifies lung recovery by computing:

1. **Jacobian Determinant Maps**: Local volume change at each voxel
   - det(J) > 1 â†’ expansion
   - det(J) = 1 â†’ no change  
   - 0 < det(J) < 1 â†’ contraction
   - det(J) â‰¤ 0 â†’ topology folding (penalized)

2. **Recovery Score**: Fraction of lung voxels with normal Jacobian (0.8â€“1.2)

3. **Recovery Classification**:
   - **Complete Recovery** (score â‰¥ 0.85)
   - **Partial Recovery** (0.50 â‰¤ score < 0.85)
   - **Persistent Abnormality** (score < 0.50)

4. **Trajectory Analysis**: Linear trend fitting across timepoints with recovery time estimation

---

## ðŸ§ª Testing

```bash
# Run all tests
python -m pytest tests/ -v

# Individual test suites
python -m pytest tests/test_model.py -v
python -m pytest tests/test_losses.py -v
python -m pytest tests/test_data.py -v
```

---

## ðŸ“„ License

This project is for academic research purposes. Datasets are subject to their respective licenses (CC BY-NC 4.0).

## ðŸ“š References

1. Balakrishnan et al., "VoxelMorph: A Learning Framework for Deformable Medical Image Registration", IEEE TMI, 2019
2. Dalca et al., "Unsupervised Learning for Probabilistic Diffeomorphic Registration for Images and Surfaces", MedIA, 2019
3. STOIC (Study of Thoracic CT in COVID-19), grand-challenge.org
