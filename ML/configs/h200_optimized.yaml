# =============================================================================
# H200 GPU Optimized Configuration
# Overrides default.yaml for NVIDIA H200 (141 GB HBM3e, Hopper Architecture)
# =============================================================================

# ---- Model (larger channels for H200 memory) ----
model:
  enc_channels: [16, 32, 64, 128, 256]
  dec_channels: [256, 128, 64, 32, 16]

# ---- Data (aggressive prefetching for H200 bandwidth) ----
data:
  volume_size: [192, 192, 192]
  num_workers: 16
  pin_memory: true
  prefetch_factor: 4

# ---- Training (H200 specific optimizations) ----
training:
  batch_size: 8                      # 2x default, H200 has 141 GB
  lr: 2.0e-4                         # Scale LR with batch size
  grad_accum_steps: 4                # Effective batch = 32
  amp_enabled: true
  amp_dtype: "bfloat16"              # Native BF16 on Hopper
  compile: true                      # torch.compile for Hopper graph optimization

# ---- Distributed (multi-GPU on H200 server) ----
distributed:
  enabled: false                     # Set true if multi-GPU H200 node
  backend: "nccl"

# ---- Hardware-specific flags (applied in code) ----
# torch.backends.cudnn.benchmark = True
# torch.set_float32_matmul_precision('high')   # Use TF32
# torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = True
